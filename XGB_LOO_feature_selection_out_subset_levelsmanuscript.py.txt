#! /usr/bin/python
import numpy as np
import time
from copy import deepcopy

from sklearn.model_selection import StratifiedShuffleSplit,StratifiedKFold
from sklearn.model_selection import GridSearchCV, RandomizedSearchCV
from sklearn.metrics import roc_curve, auc, roc_auc_score
from sklearn.preprocessing import scale,minmax_scale
from sklearn.feature_selection import SelectPercentile,f_classif,SelectFdr,VarianceThreshold
from sklearn.decomposition import PCA
from sklearn.cross_decomposition import PLSRegression
import matplotlib
matplotlib.use('agg')
import matplotlib.pylab as plt

from scipy.stats import spearmanr
from scipy.cluster import hierarchy
from collections import defaultdict
from joblib import Parallel, delayed
from sklearn.metrics import r2_score, mean_squared_error
from sklearn.utils import shuffle

import pandas as pd
import seaborn as sns

from xgboost import XGBClassifier

import os
import os.path as op

try:
    os.mkdir("output_data/")
except OSError:
    print("Directory already exists")
    
try:
    os.mkdir("output_data/perm")
except OSError:
    print("Directory {output_data/perm} already exists")

try:
    os.mkdir("output_data/gini")
except OSError:
    print("Directory {output_data/gini} already exists")
    

def raise_error(error_msg):
    exit('ERROR!: '+error_msg)
    
    
def perform_permutation_test(model,X,y,col, scoring = 'R2', n_perm = 10): 
    """
    

    Parameters
    ----------
    model : sklearn type models 
        This code has been writen using sklearn type models. It might also be 
        used for other parties models 
    X : array 
        Containing variable with on the columns the variable/features and on the
        row the subcets that will be tested. 
    y : array
        Containing the labels that will be shuffeld to train the permutated model
    col : array 
        array that contains which columns should be compaired
    scoring : str, optional
        The scroing type that will be used to compaire the initial model score 
        versus the ones that are shuffled. The default is 'R2'.
    n_perm : int, optional
        The number of thimes the columns are shuffled to compute the feature importance . The default is 10.

    Returns
    -------
    list of feature importances 
        

    """
    
    featimp = []
    if 'R2' == scoring: 
         score0 = r2_score(y, model.predict(X))
    if 'mse' == scoring: 
        score0 = mean_squared_error(y, model.predict(X))
        
    score_perm = []   
    for _ in range(n_perm):
        
        X[:,col] = shuffle(X[:,col])
        if 'R2' == scoring: 
              score_perm.append(r2_score(y, model.predict(X)) )
        if 'mse' == scoring: 
              score_perm.append(mean_squared_error(y, model.predict(X)))
       
    return abs(score0-sum(score_perm)/n_perm)


def loo_cross_validation(X, y, uni_feat_percent, feat_names, preds, mean_fpr, rand_seed=0, ncores = -1):
    from sklearn.model_selection import LeaveOneOut
    eval_metric = 'auc'
    scoring = 'roc_auc'

    from imblearn.under_sampling import NearMiss, RandomUnderSampler
    from imblearn.over_sampling import RandomOverSampler, SVMSMOTE, BorderlineSMOTE

    splitter = LeaveOneOut()

    y_preds = []
    y_preds_class = []

    param_grid_xgboost = {'max_depth': [2, 5, 7],
                          'learning_rate': [0.01, 0.1],
                          'n_estimators': [100, 300, 800, 1000, ],
                          'min_child_weight': [1, 5, ],
                          'gamma': [0.5, 2],
                          'subsample': [0.5, 0.6, 0.8],
                          'colsample_bytree': [0.6, 0.8, 1.0],
                          }
    for train, test in splitter.split(X, y):
        X_train, X_test = X[train, :], X[test, :]
        y_train, y_test = y[train], y[test]


        model = XGBClassifier(n_jobs=-1, objective='reg:squarederror', tree_method='auto')
        param_grid = param_grid_xgboost
        fit_params = {'eval_metric': eval_metric,
                      'verbose': False,
                      'early_stopping_rounds': 50,
                      'eval_set': [(X_test, y_test)]}

        skf_xgb = StratifiedShuffleSplit(train_size=0.7, test_size=0.3, n_splits=3, random_state=rand_seed)

        random_search = GridSearchCV(model, param_grid,
                                            n_jobs= ncores,
                                            cv=skf_xgb, scoring=scoring,
                                            verbose=1, refit=True)
        random_search.fit(X_train, y_train.ravel())

        best_model = random_search.best_estimator_

        imps = best_model.feature_importances_
        
        
        preds['feat_imps'].append(imps)
        preds['feat_names'].append(feat_names)
        y_preds.append(best_model.predict_proba(X_test)[:, 1])
        y_preds_class.append(best_model.predict(X_test))
    from sklearn.metrics import confusion_matrix
    print(confusion_matrix(y, y_preds_class))
    score = roc_auc_score(y, y_preds)
    fpr, tpr, _ = roc_curve(y, y_preds, pos_label=1)
    preds['score_list']['list_tprs'].append(np.interp(mean_fpr, fpr, tpr))
    preds['score_list']['list_tprs'][-1][0] = 0.0
    preds['score_list']['auc'].append(score)

    preds['preds'].append(y_preds)
    preds['y_test'].append(y)

    print('*' * 15)
    print(f'\n\n\n {scoring.upper()}: {score}\n\n\n')
    print('*' * 15)

    return preds


def average_loo(X_raw, y, uni_feat_percent, feat_names_full, preds, results_path, mean_fpr, y_df, n_runs=20, subset_size = 0.5, subset_frac=0.5, rand_seed=0):

    preds_loo = deepcopy(preds)

    np.random.seed(rand_seed)
    
    b = SelectPercentile(f_classif,uni_feat_percent)
    b.fit(X_raw,y)
    X_raw=X_raw[:, b.get_support(indices=True)]
    feat_names=feat_names_full[b.get_support(indices=True)]
    

    for run in range(n_runs):
        random_selection = np.random.choice(np.arange(X_raw.shape[0]), subset_size, replace=False)
        loo_cross_validation(X_raw[random_selection, :], y[random_selection], uni_feat_percent, 
                             feat_names, preds_loo, mean_fpr)

    avg_auc = np.mean(preds_loo['score_list']['auc'])
    
    np.save(results_path + '_loo', preds_loo)
    
    return preds_loo

def main():
    startTime = time.time()
    print('Data loading')
    print('')
    np.random.seed(256)
    data_type='RF_Class'
    stability_samples_run_number=20
    num_top_feat=20
    uni_feat_percent=(100/816)*50
    X_train_df=pd.read_excel(io='pre/x_train_pre_df.xlsx', index_col='SAMPLE_ID')
    y_train_df=pd.read_excel(io='pre/y_train_pre_df.xlsx', index_col='SAMPLE_ID')
    X_train_df.fillna(method='ffill',inplace=True)

    X_raw=np.asarray(X_train_df.values,dtype=float)
    y_float=np.asarray(y_train_df.iloc[:,0].values,dtype=float)
    y_int=np.asarray(y_float,dtype=int)
    label_names={0:'Autologous',1:'Allogenic'}
    y_verbal=np.asarray([label_names[i] for i in y_int],dtype=str)
    b = SelectPercentile(f_classif,uni_feat_percent)
    
    print("")
    b.fit(X_raw,y_int)

    y=y_float
    y_df=y_train_df.reset_index(drop=True)
    

    feat_names_full=np.asarray(list(X_train_df.columns),dtype=str)
    example_names=np.asarray(list(X_train_df.index),dtype=str)

    mean_fpr = np.linspace(0, 1, 100)

    i = 0
    print('Calculating Stratified Shuffled Split coefficients:')

    plt.style.use('ggplot')
    params = {'legend.fontsize': 'medium',
              'figure.figsize': (10, 10),
              'axes.labelsize': 'medium',
              'axes.titlesize':'medium',
              'xtick.labelsize':'medium',
              'ytick.labelsize':'medium'}
    plt.rcParams.update(params)
    plt.rcParams["font.family"] = "sans-serif"
    
    running = True

    results_path = op.join(os.getcwd(), 'output_data', 'results_dict')

    if running:
        

        score_list_dict = {'auc': [], 'list_tprs': []}
    
        results_dict = {'score_list': deepcopy(score_list_dict), 'preds': [], 'feat_imps': [], 'y_test': [], 
                        'feat_names':[]}
    

        subset_size= int(.85 * X_raw.shape[0])
            
        results_dict = average_loo(X_raw, y_int, uni_feat_percent, feat_names_full, results_dict,
                    results_path, mean_fpr, y_df = y_df, n_runs=stability_samples_run_number,
                    subset_size = subset_size)
        
    else:
        results_dict = np.load(results_path + '_loo.npy', allow_pickle=True).all()

    list_feat_importances = results_dict['feat_imps']
    tprs = results_dict['score_list']['list_tprs']
    aucs =  results_dict['score_list']['auc']

    feat_names = results_dict['feat_names']
    fi = pd.DataFrame(list_feat_importances)
    fi = pd.Series(fi.values.ravel('F'), name="feat_imps")
    feat_names_dict = pd.DataFrame(feat_names)
    feat_names_dict = pd.Series(feat_names_dict.values.ravel('F'), name="feat_names")
    
    fi = pd.concat([feat_names_dict, fi], axis = 1)
    fi_avg = fi.groupby(["feat_names"], as_index=False).mean()
    
    feature_importance = fi_avg['feat_imps']

    feature_importance = np.round(100.0 * (feature_importance / np.nanmax(feature_importance)),2)
    sorted_idx = np.argsort(feature_importance)
    top_sorted_idx=sorted_idx[-30:]
    pos = np.arange(top_sorted_idx.shape[0]) + .5

    plt.plot([0, 1], [0, 1], linestyle='--', lw=2, color='r',
             label='Luck', alpha=.8)

    mean_tpr = np.mean(tprs, axis=0)
    mean_tpr[-1] = 1.0
    mean_auc = auc(mean_fpr, mean_tpr)
    std_auc = np.std(aucs)
    plt.plot(mean_fpr, mean_tpr, color='b',
             label=r'Mean ROC (AUC = %0.2f $\pm$ %0.2f)' % (mean_auc, std_auc),
             lw=2, alpha=.8)

    std_tpr = np.std(tprs, axis=0)
    tprs_upper = np.minimum(mean_tpr + std_tpr, 1)
    tprs_lower = np.maximum(mean_tpr - std_tpr, 0)
    plt.fill_between(mean_fpr, tprs_lower, tprs_upper, color='grey', alpha=.2,
                     label=r'$\pm$ 1 std. dev.')

    plt.xlim([-0.05, 1.05])
    plt.ylim([-0.05, 1.05])
    plt.xlabel('False Positive Rate')
    plt.ylabel('True Positive Rate')
    plt.title('Receiver operating characteristic example')
    plt.legend(loc="lower right")
    plt.savefig('output_data/auc_avg'+data_type+'.pdf', bbox_inches='tight')
    plt.close()

    plt.figure()
    plt.barh(pos, feature_importance[top_sorted_idx], align='center')
    plt.yticks(pos, fi_avg["feat_names"][top_sorted_idx])
    plt.xlabel('Relative Importance')
    plt.title('Top 30 Relative Variable Importances')
    plt.savefig('output_data/feat_imp_gini.pdf', bbox_inches='tight')
    plt.close()
    
    header=np.reshape(np.asarray(['FeatName','RelFeatImp']),(1,2))
    feat_imp_vector=pd.concat([fi_avg["feat_names"], feature_importance], axis = 1)
    feat_imp_vector=feat_imp_vector.sort_values(by="feat_imps", ascending=False)
    feat_vector_save=np.vstack((header,feat_imp_vector))
    
    np.savetxt('output_data/feat_imp.txt',feat_vector_save,fmt='%s',delimiter='\t')

    dataset_df=pd.DataFrame(data=X_raw,index=example_names,columns=feat_names_full)
    dataset_df['class_label']=y_int
    dataset_df['class_label_verbal']=y_verbal
    sel_features=np.loadtxt('output_data/feat_imp.txt',dtype=bytes,delimiter ='\t',skiprows=1).astype(str)
    feat_scores=np.asarray(sel_features[:,1],dtype=float)
    feat_scores_sorted_idx=np.argsort(feat_scores)[::-1]
    feat_scores_sorted=feat_scores[feat_scores_sorted_idx]
    top_feat_names_sorted=sel_features[feat_scores_sorted_idx[:num_top_feat],0]
    
    box_count=0
    for feat_name in top_feat_names_sorted:
        box_count+=1
        plt.figure(figsize=(10,10))
        bplot=sns.boxplot(y=feat_name, x='class_label_verbal',
                         data=dataset_df,
                         width=0.5,
                         palette="colorblind",
                         showfliers=False)
        bplot=sns.stripplot(y=feat_name, x='class_label_verbal',
                           data=dataset_df,
                           jitter=True,
                           marker='o',
                           alpha=0.5,
                           color='black')
        plot_file_name="output_data/gini/boxplot_gini"+"_"+str(box_count)+".png"
        bplot.figure.savefig(plot_file_name,
                            format='png',
                            dpi=100)

    X_pca=X_raw[:,feat_scores_sorted_idx[:num_top_feat]]
    y_plsr=y
    pca=PCA(n_components=2)
    X_r=pca.fit(X_pca).transform(X_pca)
    plsr=PLSRegression(n_components=2)
    X_r2,y_r2=plsr.fit_transform(X_pca, y_plsr)
    pca_var=pca.explained_variance_ratio_

    plt.figure(figsize=(20, 10))
    plt.subplot(121)
    colors=['navy', 'red']
    lw=2
    for color, i, target_name in zip(colors, [0, 1], [label_names[0],label_names[1]]):
        plt.scatter(X_r[y == i, 0], X_r[y == i, 1], color=color, alpha=.8, lw=lw,
                    label=target_name)
    plt.xlabel('Explained Variance '+str(np.round(100*pca_var[0],2))+'%')
    plt.ylabel('Explained Variance '+str(np.round(100*pca_var[1],2))+'%')
    plt.legend(loc='best', shadow=False, scatterpoints=1)
    plt.title('PCA of top '+str(num_top_feat)+' selected features dataset')

    plt.subplot(122)
    for color, i, target_name in zip(colors, [0, 1], [label_names[0],label_names[1]]):
        plt.scatter(X_r2[y == i, 0], X_r2[y == i, 1], alpha=.8, color=color,
                    label=target_name)
    plt.legend(loc='best', shadow=False, scatterpoints=1)
    plt.xlabel('Score 1')
    plt.ylabel('Score 2')
    plt.title('PLS-DA of top '+str(num_top_feat)+' selected features dataset')
    plt.savefig("output_data/scatter_pca.png",
                         format='png',
                         dpi=100)

    endTime = time.time()
    runTime=endTime-startTime
    print(('Runtime: %.2f seconds' %runTime))

if __name__ == "__main__":
    __spec__ = None
    dummy=main()
